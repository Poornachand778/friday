{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812911b5",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è DEPRECATED NOTEBOOK\n",
    "\n",
    "**This notebook is outdated.** Use the production-ready Python scripts instead:\n",
    "\n",
    "```bash\n",
    "# Multi-GPU training (recommended)\n",
    "python scripts/train_multigpu.py\n",
    "\n",
    "# Single GPU (memory optimized)  \n",
    "python scripts/train_memory_diet.py\n",
    "```\n",
    "\n",
    "See the main [README.md](../README.md) for current setup instructions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb52ae",
   "metadata": {},
   "source": [
    "# Friday AI Assistant - LoRA Fine-tuning on SageMaker\n",
    "\n",
    "This notebook demonstrates how to fine-tune Meta-Llama-3.1-8B-Instruct using LoRA on AWS SageMaker for your Friday AI assistant.\n",
    "\n",
    "## Prerequisites\n",
    "1. AWS SageMaker Studio or Notebook Instance\n",
    "2. Proper IAM permissions for SageMaker, S3\n",
    "3. Your training data uploaded to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae52d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q sagemaker boto3 huggingface_hub transformers datasets peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef25349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "BUCKET_NAME = \"friday-ai-training\"  # Change this to your S3 bucket\n",
    "REGION = \"us-east-1\"  # Change to your region\n",
    "S3_PREFIX = \"friday-finetuning\"\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"S3 bucket: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbf6b8b",
   "metadata": {},
   "source": [
    "## Step 1: Verify Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e78d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data exists in S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def check_s3_file(bucket, key):\n",
    "    try:\n",
    "        response = s3.head_object(Bucket=bucket, Key=key)\n",
    "        size = response['ContentLength']\n",
    "        print(f\"‚úÖ Found: s3://{bucket}/{key} ({size:,} bytes)\")\n",
    "        return True\n",
    "    except:\n",
    "        print(f\"‚ùå Missing: s3://{bucket}/{key}\")\n",
    "        return False\n",
    "\n",
    "# Check training files\n",
    "train_exists = check_s3_file(BUCKET_NAME, f\"{S3_PREFIX}/data/train.jsonl\")\n",
    "valid_exists = check_s3_file(BUCKET_NAME, f\"{S3_PREFIX}/data/valid.jsonl\")\n",
    "\n",
    "if not (train_exists and valid_exists):\n",
    "    print(\"\\n‚ùå Please upload your training data first using the prepare_sagemaker_data.py script\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All training data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8aa45f",
   "metadata": {},
   "source": [
    "## Step 2: Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875401a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training script\n",
    "training_script = \"\"\"\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import argparse\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \\\"\\\"\\\"Load JSONL dataset\\\"\\\"\\\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def format_messages(example):\n",
    "    \\\"\\\"\\\"Format messages into training text\\\"\\\"\\\"\n",
    "    messages = example[\\\"messages\\\"]\n",
    "    text = \\\"\\\"\n",
    "    for msg in messages:\n",
    "        if msg[\\\"role\\\"] == \\\"system\\\":\n",
    "            text += f\\\"<|start_header_id|>system<|end_header_id|>\\\\n{msg['content']}<|eot_id|>\\\"\n",
    "        elif msg[\\\"role\\\"] == \\\"user\\\":\n",
    "            text += f\\\"<|start_header_id|>user<|end_header_id|>\\\\n{msg['content']}<|eot_id|>\\\"\n",
    "        elif msg[\\\"role\\\"] == \\\"assistant\\\":\n",
    "            text += f\\\"<|start_header_id|>assistant<|end_header_id|>\\\\n{msg['content']}<|eot_id|>\\\"\n",
    "    return {\\\"text\\\": text}\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\\\"--model-name\\\", type=str, default=\\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\")\n",
    "    parser.add_argument(\\\"--train-data\\\", type=str, default=\\\"/opt/ml/input/data/training/train.jsonl\\\")\n",
    "    parser.add_argument(\\\"--valid-data\\\", type=str, default=\\\"/opt/ml/input/data/training/valid.jsonl\\\")\n",
    "    parser.add_argument(\\\"--output-dir\\\", type=str, default=\\\"/opt/ml/model\\\")\n",
    "    parser.add_argument(\\\"--epochs\\\", type=int, default=3)\n",
    "    parser.add_argument(\\\"--batch-size\\\", type=int, default=2)\n",
    "    parser.add_argument(\\\"--learning-rate\\\", type=float, default=2e-4)\n",
    "    parser.add_argument(\\\"--lora-rank\\\", type=int, default=16)\n",
    "    parser.add_argument(\\\"--lora-alpha\\\", type=int, default=32)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(f\\\"üöÄ Starting training with {args.model_name}\\\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\\\"auto\\\",\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=args.lora_rank,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\\\"q_proj\\\", \\\"k_proj\\\", \\\"v_proj\\\", \\\"o_proj\\\", \\\"gate_proj\\\", \\\"up_proj\\\", \\\"down_proj\\\"]\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Load and process datasets\n",
    "    print(\\\"üìä Loading datasets...\\\")\n",
    "    train_data = load_dataset(args.train_data)\n",
    "    valid_data = load_dataset(args.valid_data)\n",
    "    \n",
    "    print(f\\\"üìà Train examples: {len(train_data)}\\\")\n",
    "    print(f\\\"üìä Valid examples: {len(valid_data)}\\\")\n",
    "    \n",
    "    train_dataset = Dataset.from_list(train_data).map(format_messages)\n",
    "    valid_dataset = Dataset.from_list(valid_data).map(format_messages)\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\\\"text\\\"],\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=2048,\n",
    "            return_tensors=\\\"pt\\\"\n",
    "        )\n",
    "    \n",
    "    print(\\\"üîß Tokenizing datasets...\\\")\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=100,\n",
    "        learning_rate=args.learning_rate,\n",
    "        bf16=True,\n",
    "        logging_steps=10,\n",
    "        eval_steps=50,\n",
    "        save_steps=100,\n",
    "        evaluation_strategy=\\\"steps\\\",\n",
    "        save_strategy=\\\"steps\\\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\\\"eval_loss\\\",\n",
    "        greater_is_better=False,\n",
    "        report_to=None,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\\\"üéØ Starting training...\\\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model\n",
    "    print(\\\"üíæ Saving model...\\\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    \n",
    "    print(\\\"‚úÖ Training completed!\\\")\n",
    "\n",
    "if __name__ == \\\"__main__\\\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# Write the script to a file\n",
    "with open(\"train.py\", \"w\") as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "print(\"‚úÖ Training script created: train.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14469e1a",
   "metadata": {},
   "source": [
    "## Step 3: Configure Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31490d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training job configuration\n",
    "hyperparameters = {\n",
    "    \"epochs\": 3,\n",
    "    \"batch-size\": 2,\n",
    "    \"learning-rate\": 2e-4,\n",
    "    \"lora-rank\": 16,\n",
    "    \"lora-alpha\": 32,\n",
    "}\n",
    "\n",
    "# Data inputs\n",
    "training_input_path = f\"s3://{BUCKET_NAME}/{S3_PREFIX}/data\"\n",
    "output_path = f\"s3://{BUCKET_NAME}/{S3_PREFIX}/output\"\n",
    "\n",
    "print(f\"üìÅ Training data: {training_input_path}\")\n",
    "print(f\"üì§ Output path: {output_path}\")\n",
    "print(f\"üîß Hyperparameters: {hyperparameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6caa9a",
   "metadata": {},
   "source": [
    "## Step 4: Create HuggingFace Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175cb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\".\",\n",
    "    instance_type=\"ml.g5.2xlarge\",  # GPU instance for faster training\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version=\"4.36.0\",\n",
    "    pytorch_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_path,\n",
    "    max_run=3*60*60,  # 3 hours max\n",
    "    volume_size=100,  # GB\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\",\n",
    "        \"HF_HOME\": \"/tmp/.cache\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ HuggingFace estimator created\")\n",
    "print(f\"üí∞ Instance type: ml.g5.2xlarge (estimated cost: ~$1.50/hour)\")\n",
    "print(f\"‚è±Ô∏è  Max runtime: 3 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd3792",
   "metadata": {},
   "source": [
    "## Step 5: Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aafe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training job\n",
    "job_name = f\"friday-lora-{int(time.time())}\"\n",
    "\n",
    "print(f\"üöÄ Starting training job: {job_name}\")\n",
    "print(f\"üìä Monitor progress at: https://console.aws.amazon.com/sagemaker/home?region={REGION}#/jobs/{job_name}\")\n",
    "\n",
    "huggingface_estimator.fit(\n",
    "    {\"training\": training_input_path},\n",
    "    job_name=job_name,\n",
    "    wait=False  # Set to True if you want to wait for completion\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training job submitted!\")\n",
    "print(f\"üîó Job name: {job_name}\")\n",
    "print(f\"üìà You can monitor the job in the SageMaker console\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c460a",
   "metadata": {},
   "source": [
    "## Step 6: Monitor Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor training job status\n",
    "import time\n",
    "\n",
    "def monitor_training(estimator, check_interval=60):\n",
    "    \"\"\"Monitor training job status\"\"\"\n",
    "    while True:\n",
    "        status = estimator.latest_training_job.describe()['TrainingJobStatus']\n",
    "        print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Status: {status}\")\n",
    "        \n",
    "        if status in ['Completed', 'Failed', 'Stopped']:\n",
    "            break\n",
    "            \n",
    "        time.sleep(check_interval)\n",
    "    \n",
    "    if status == 'Completed':\n",
    "        print(\"\\n‚úÖ Training completed successfully!\")\n",
    "        print(f\"üì§ Model artifacts: {estimator.model_data}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Training {status.lower()}\")\n",
    "\n",
    "# Uncomment to monitor the job\n",
    "# monitor_training(huggingface_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba023f",
   "metadata": {},
   "source": [
    "## Step 7: Deploy Model (After Training Completes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the trained model (run this after training completes)\n",
    "# predictor = huggingface_estimator.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type=\"ml.g5.xlarge\",\n",
    "#     endpoint_name=f\"friday-endpoint-{int(time.time())}\"\n",
    "# )\n",
    "\n",
    "# print(f\"‚úÖ Model deployed to endpoint: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe6567",
   "metadata": {},
   "source": [
    "## Estimated Costs\n",
    "\n",
    "**Training (ml.g5.2xlarge):**\n",
    "- ~$1.50/hour\n",
    "- Expected training time: 1-2 hours\n",
    "- **Total training cost: ~$2-3**\n",
    "\n",
    "**Inference (ml.g5.xlarge):**\n",
    "- ~$0.75/hour\n",
    "- Only when endpoint is running\n",
    "\n",
    "**Storage (S3):**\n",
    "- ~$0.02/GB/month\n",
    "- Model artifacts ~10-20GB\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Run this notebook in SageMaker Studio\n",
    "2. Monitor training in SageMaker console\n",
    "3. Download model artifacts after training\n",
    "4. Deploy for inference or use locally"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
